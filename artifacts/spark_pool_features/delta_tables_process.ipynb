{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Use Case:\r\n",
        "Delta Lake is one of the most popular file formats in Azure Data Lakes. It allows you to apply ACID transactions to your fils on the lake and perform operations as update, delete and merge. It also provides time travel capabilities to look at historical data.\r\n",
        "This sample Notebook shows you how to create, update, and query a Delta Lake table.\r\n",
        "Documentation: https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-delta-lake-overview?pivots=programming-language-python#sql-support"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\r\n",
        "First we create the Delta Table. We will use a copy of the Fact Call Center Parquet file and include a couple of partition columns"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Variables used on this example\r\n",
        "synapse_data_lake = \"REPLACE_DATALAKE_NAME\"\r\n",
        "source_path = \"abfss://data@\" + synapse_data_lake + \".dfs.core.windows.net/Sample/AdventureWorks/FactCallCenter/\"\r\n",
        "target_path = \"abfss://data@\" + synapse_data_lake + \".dfs.core.windows.net/Sample/AdventureWorks/FactCallCenter_Delta/\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First we will create a copy of the sample data\r\n",
        "# and we will convert the copy from Parquet to Delta Lake Format\r\n",
        "# adding Partition columns in the process\r\n",
        "from delta.tables import *\r\n",
        "df = spark.read.format('parquet').load(source_path)\r\n",
        "df.write.partitionBy('WageType', 'Shift').format('delta').save(target_path)\r\n",
        "# Register the FactCallCenter_Delta table.\r\n",
        "spark.sql(\"CREATE DATABASE DeltaTest\")\r\n",
        "spark.sql(\"CREATE TABLE DeltaTest.FactCallCenter_Delta USING DELTA LOCATION '\" + target_path + \"'\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query the Delta Table\r\n",
        "Now you can use Spark or SQL to query the data stored in your delta table. Notice you don't have to do anything special to work with the partition folders as both Synapse Spark and Synapse SQL Serverless Pools understand Delta lake partitions."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now you can query the new Delta Table using PySpark\r\n",
        "spark.sql(\"SELECT WageType, COUNT(*) FROM DeltaTest.FactCallCenter_Delta GROUP BY WageType\").show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\r\n",
        "-- You can also run queries using Spark SQL\r\n",
        "SELECT WageType, COUNT(*) \r\n",
        "FROM DeltaTest.FactCallCenter_Delta \r\n",
        "GROUP BY WageType "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Synapse SQL Serverless Pool and T-SQL\r\n",
        "Now you can open a new SQL Script Windows and use Serverless to query the newly created Delta Table.\r\n",
        "Synapse takes care of \"sharing\" the Spark table definition with the Synapse SQL Serverless engine.\r\n",
        "Notice the use of the **dbo** schema\r\n",
        "```sql\r\n",
        "SELECT WageType, COUNT(*) \r\n",
        "FROM DeltaTest.dbo.FactCallCenter_Delta \r\n",
        "GROUP BY WageType \r\n",
        "```"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Update Data\r\n",
        "This example show you how to perform append, update, and delete operations on your delta lake and how to use time travel to look at previous versions of your table.\r\n",
        "On this example, We will limit the value stored in the calls column to 500 ( update all records with a calls value > 500 )"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\r\n",
        "-- First we check the number of records we will update:\r\n",
        "SELECT COUNT(*)\r\n",
        "FROM DeltaTest.FactCallCenter_Delta \r\n",
        "WHERE calls > 500 "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\r\n",
        "-- Now we update the records\r\n",
        "UPDATE DeltaTest.FactCallCenter_Delta \r\n",
        "    SET calls = 500\r\n",
        "WHERE calls > 500 "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\r\n",
        "-- If we check again, we shouldn't have any records with calls > 500\r\n",
        "SELECT COUNT(*)\r\n",
        "FROM DeltaTest.FactCallCenter_Delta \r\n",
        "WHERE calls > 500 "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\r\n",
        "-- We also want to delete all the records where TotalOperators is equal to 12\r\n",
        "-- First, we check how many records we will delete\r\n",
        "SELECT COUNT(*) FROM DeltaTest.FactCallCenter_Delta \r\n",
        "WHERE TotalOperators == 12"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\r\n",
        "-- Now we proceed to delete the records where TotalOperators is 12 \r\n",
        "DELETE FROM DeltaTest.FactCallCenter_Delta \r\n",
        "WHERE TotalOperators == 12"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\r\n",
        "-- If we check again, there shouldn't be any records with that condition\r\n",
        "SELECT COUNT(*) FROM DeltaTest.FactCallCenter_Delta \r\n",
        "WHERE TotalOperators == 12"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Time Travel Operations\r\n",
        "Let's look at how we can see the different versions of our table before and after the Update and Delete operations"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\r\n",
        "-- We should be able to see the 3 versions of the table:\r\n",
        "-- Version 0 represents the creation of the Delta Table, \r\n",
        "-- Version 1 represents the table after the update of the calls columns\r\n",
        "-- Version 2 represents the table after records were deleted\r\n",
        "DESCRIBE HISTORY DeltaTest.FactCallCenter_Delta"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we use time travel to get table counts before and after the Delete operation\r\n",
        "df = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(target_path)\r\n",
        "before_delete = df.count()\r\n",
        "df = spark.read.format(\"delta\").option(\"versionAsOf\", 2).load(target_path)\r\n",
        "after_delete = df.count()\r\n",
        "print(f'Records before delete:{before_delete}')\r\n",
        "print(f'Records after delete:{after_delete}')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {}
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clean Up\r\n",
        "We will now proceed to delete the delta table and the folder created"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from notebookutils import mssparkutils\r\n",
        "# Delete Delta Table definitions\r\n",
        "spark.sql(\"DROP TABLE DeltaTest.FactCallCenter_Delta\")\r\n",
        "spark.sql(\"DROP DATABASE DeltaTest\")\r\n",
        "# Delete Delta Folder\r\n",
        "mssparkutils.fs.rm(target_path, recurse=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": false,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}